{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4AcT8P3IBtwN",
        "outputId": "86f0ba55-81f0-480a-f941-0f3230905940"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (2.32.3)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (4.13.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/dist-packages (4.1.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Collecting pinecone\n",
            "  Downloading pinecone-7.0.2-py3-none-any.whl.metadata (9.5 kB)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests) (2025.4.26)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4) (2.7)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4) (4.13.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.5.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.52.2)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (2.6.0+cu124)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.15.3)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (0.31.4)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (11.2.1)\n",
            "Collecting pinecone-plugin-assistant<2.0.0,>=1.6.0 (from pinecone)\n",
            "  Downloading pinecone_plugin_assistant-1.6.1-py3-none-any.whl.metadata (27 kB)\n",
            "Collecting pinecone-plugin-interface<0.0.8,>=0.0.7 (from pinecone)\n",
            "  Downloading pinecone_plugin_interface-0.0.7-py3-none-any.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.58.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (24.2)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.3.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
            "Downloading pinecone-7.0.2-py3-none-any.whl (516 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m516.3/516.3 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pinecone_plugin_assistant-1.6.1-py3-none-any.whl (239 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m239.1/239.1 kB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pinecone_plugin_interface-0.0.7-py3-none-any.whl (6.2 kB)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m24.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m26.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m19.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m73.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pinecone-plugin-interface, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, pinecone-plugin-assistant, nvidia-cusparse-cu12, nvidia-cudnn-cu12, pinecone, nvidia-cusolver-cu12\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 pinecone-7.0.2 pinecone-plugin-assistant-1.6.1 pinecone-plugin-interface-0.0.7\n"
          ]
        }
      ],
      "source": [
        "!pip install requests pandas beautifulsoup4 tqdm nltk sentence-transformers numpy pinecone matplotlib"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall pinecone-client -y\n",
        "!pip install pinecone"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RNp0AjnxDVoX",
        "outputId": "c4275cae-c46d-4e3a-e91e-ba129171a723"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: pinecone-client 6.0.0\n",
            "Uninstalling pinecone-client-6.0.0:\n",
            "  Successfully uninstalled pinecone-client-6.0.0\n",
            "Collecting pinecone\n",
            "  Using cached pinecone-7.0.2-py3-none-any.whl.metadata (9.5 kB)\n",
            "Requirement already satisfied: certifi>=2019.11.17 in /usr/local/lib/python3.11/dist-packages (from pinecone) (2025.4.26)\n",
            "Requirement already satisfied: pinecone-plugin-assistant<2.0.0,>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from pinecone) (1.6.1)\n",
            "Requirement already satisfied: pinecone-plugin-interface<0.0.8,>=0.0.7 in /usr/local/lib/python3.11/dist-packages (from pinecone) (0.0.7)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.11/dist-packages (from pinecone) (2.9.0.post0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.11/dist-packages (from pinecone) (4.13.2)\n",
            "Requirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from pinecone) (2.4.0)\n",
            "Requirement already satisfied: packaging<25.0,>=24.2 in /usr/local/lib/python3.11/dist-packages (from pinecone-plugin-assistant<2.0.0,>=1.6.0->pinecone) (24.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.32.3 in /usr/local/lib/python3.11/dist-packages (from pinecone-plugin-assistant<2.0.0,>=1.6.0->pinecone) (2.32.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.5.3->pinecone) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.32.3->pinecone-plugin-assistant<2.0.0,>=1.6.0->pinecone) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.32.3->pinecone-plugin-assistant<2.0.0,>=1.6.0->pinecone) (3.10)\n",
            "Using cached pinecone-7.0.2-py3-none-any.whl (516 kB)\n",
            "Installing collected packages: pinecone\n",
            "Successfully installed pinecone-7.0.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "from bs4 import BeautifulSoup\n",
        "import json\n",
        "from tqdm import tqdm\n",
        "import re\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import numpy as np\n",
        "import pinecone\n",
        "import matplotlib.pyplot as plt\n",
        "import time"
      ],
      "metadata": {
        "id": "jkgWheagB0Ue"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pinecone import Pinecone, ServerlessSpec"
      ],
      "metadata": {
        "id": "I1h1Ei7_PP-8"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "GROQ_API_KEY = \"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\"\n",
        "client = Groq(api_key=GROQ_API_KEY)"
      ],
      "metadata": {
        "id": "_3IKKRgmMFlR"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "try:\n",
        "    nltk.data.find('tokenizers/punkt')\n",
        "except LookupError:\n",
        "    print(\"Downloading punkt tokenizer...\")\n",
        "    nltk.download('punkt', quiet=True)\n",
        "    print(\"Punkt tokenizer downloaded.\")"
      ],
      "metadata": {
        "id": "R29V5tCbB4Lw"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Assignment 2"
      ],
      "metadata": {
        "id": "FrFG5MfCrAvD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def collect_data():\n",
        "    \"\"\"\n",
        "    Collect data from Wikipedia and custom sources in both English and Urdu\n",
        "    \"\"\"\n",
        "    print(\"\\n=== Step 3: Collecting Data ===\")\n",
        "\n",
        "    # English articles to collect\n",
        "    en_articles = [\n",
        "        'Nutrition',\n",
        "        'Healthy_diet',\n",
        "        'Pakistani_cuisine',\n",
        "        'Indian_cuisine',\n",
        "        'South_Asian_cuisine',\n",
        "        'Halal',\n",
        "        'Desi_cuisine'\n",
        "    ]\n",
        "\n",
        "    # Urdu articles to collect (reduced list to more likely existing articles)\n",
        "    ur_articles = [\n",
        "        'Pakistani_cuisine',\n",
        "        'Indian_cuisine'\n",
        "    ]\n",
        "\n",
        "    def get_wikipedia_content(title, lang='en', max_retries=3, delay=2):\n",
        "        for attempt in range(max_retries):\n",
        "            try:\n",
        "                url = f'https://{lang}.wikipedia.org/w/api.php'\n",
        "                params = {\n",
        "                    'action': 'query',\n",
        "                    'format': 'json',\n",
        "                    'titles': title,\n",
        "                    'prop': 'extracts',\n",
        "                    'exintro': True,\n",
        "                    'explaintext': True\n",
        "                }\n",
        "                headers = {\n",
        "                    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
        "                }\n",
        "                response = requests.get(url, params=params, headers=headers)\n",
        "                response.raise_for_status()\n",
        "\n",
        "                try:\n",
        "                    data = response.json()\n",
        "                except json.JSONDecodeError:\n",
        "                    print(f\"Error: Invalid JSON response for {title} in {lang}\")\n",
        "                    if attempt < max_retries - 1:\n",
        "                        time.sleep(delay)\n",
        "                        continue\n",
        "                    return None\n",
        "\n",
        "                pages = data['query']['pages']\n",
        "                page = next(iter(pages.values()))\n",
        "\n",
        "                if 'extract' not in page:\n",
        "                    print(f\"Warning: No content found for {title} in {lang}\")\n",
        "                    return None\n",
        "\n",
        "                return page['extract']\n",
        "\n",
        "            except requests.exceptions.RequestException as e:\n",
        "                print(f\"Error fetching {title} in {lang}: {str(e)}\")\n",
        "                if attempt < max_retries - 1:\n",
        "                    print(f\"Retrying in {delay} seconds...\")\n",
        "                    time.sleep(delay)\n",
        "                    continue\n",
        "                return None\n",
        "            except Exception as e:\n",
        "                print(f\"Unexpected error for {title} in {lang}: {str(e)}\")\n",
        "                if attempt < max_retries - 1:\n",
        "                    time.sleep(delay)\n",
        "                    continue\n",
        "                return None\n",
        "\n",
        "    # Additional Urdu food and nutrition data\n",
        "    urdu_food_data = [\n",
        "        {\n",
        "            'title': 'Pakistani Food',\n",
        "            'content': \"\"\"\n",
        "            پاکستانی کھانوں میں بریانی، کڑاہی، نہاری، چپلی کباب، سموسے، پکوڑے، حلیم،\n",
        "            اور مختلف قسم کی روٹیاں شامل ہیں۔ یہ کھانے صحت بخش اجزاء سے بنائے جاتے ہیں\n",
        "            اور متوازن غذا کا حصہ ہیں۔\n",
        "            \"\"\",\n",
        "            'language': 'ur'\n",
        "        },\n",
        "        {\n",
        "            'title': 'Healthy Eating',\n",
        "            'content': \"\"\"\n",
        "            صحت مند کھانے میں تازہ سبزیاں، پھل، دالیں، اور کم چکنائی والی پروٹین شامل ہونی چاہئیں۔\n",
        "            روزانہ کم از کم 8 گلاس پانی پینا چاہیے اور پروسیسڈ فوڈز سے پرہیز کرنا چاہیے۔\n",
        "            \"\"\",\n",
        "            'language': 'ur'\n",
        "        },\n",
        "        {\n",
        "            'title': 'Traditional Pakistani Diet',\n",
        "            'content': \"\"\"\n",
        "            روایتی پاکستانی غذا میں چاول، روٹی، دالیں، سبزیاں، اور گوشت شامل ہیں۔\n",
        "            یہ غذا توانائی اور غذائیت سے بھرپور ہوتی ہے۔\n",
        "            \"\"\",\n",
        "            'language': 'ur'\n",
        "        },\n",
        "        {\n",
        "            'title': 'Nutritional Benefits',\n",
        "            'content': \"\"\"\n",
        "            پاکستانی کھانوں میں موجود اجزاء صحت کے لیے بہت فائدہ مند ہیں۔\n",
        "            دالیں پروٹین کا اچھا ذریعہ ہیں، سبزیاں وٹامنز اور معدنیات سے بھرپور ہیں،\n",
        "            اور گوشت آئرن اور پروٹین فراہم کرتا ہے۔\n",
        "            \"\"\",\n",
        "            'language': 'ur'\n",
        "        },\n",
        "        {\n",
        "            'title': 'Balanced Diet',\n",
        "            'content': \"\"\"\n",
        "            متوازن غذا میں کاربوہائیڈریٹس، پروٹین، چکنائی، وٹامنز اور معدنیات کی مناسب مقدار ہونی چاہیے۔\n",
        "            پاکستانی کھانوں میں یہ تمام اجزاء موجود ہیں، لیکن اعتدال میں کھانا ضروری ہے۔\n",
        "            \"\"\",\n",
        "            'language': 'ur'\n",
        "        },\n",
        "        {\n",
        "            'title': 'Cooking Methods',\n",
        "            'content': \"\"\"\n",
        "            پاکستانی کھانوں میں مختلف طریقے استعمال ہوتے ہیں جیسے بھوننا، پکانا، اور تڑکا لگانا۔\n",
        "            یہ طریقے کھانوں کی غذائیت کو برقرار رکھتے ہیں اور ذائقہ بڑھاتے ہیں۔\n",
        "            \"\"\",\n",
        "            'language': 'ur'\n",
        "        }\n",
        "    ]\n",
        "\n",
        "    # Collect English Wikipedia articles\n",
        "    en_data = []\n",
        "    for article in tqdm(en_articles, desc='Collecting English articles'):\n",
        "        content = get_wikipedia_content(article, 'en')\n",
        "        if content:  # Only add if content was successfully retrieved\n",
        "            en_data.append({\n",
        "                'title': article,\n",
        "                'content': content,\n",
        "                'language': 'en'\n",
        "            })\n",
        "\n",
        "    # Collect Urdu content\n",
        "    ur_data = []\n",
        "    for article in tqdm(ur_articles, desc='Collecting Urdu articles'):\n",
        "        content = get_wikipedia_content(article, 'ur')\n",
        "        if content:  # Only add if content was successfully retrieved\n",
        "            ur_data.append({\n",
        "                'title': article,\n",
        "                'content': content,\n",
        "                'language': 'ur'\n",
        "            })\n",
        "\n",
        "    # Add additional Urdu food data\n",
        "    ur_data.extend(urdu_food_data)\n",
        "\n",
        "    # Combine all data\n",
        "    all_data = en_data + ur_data\n",
        "    df = pd.DataFrame(all_data)\n",
        "    print(f\"\\nData Collection Summary:\")\n",
        "    print(f\"Total documents collected: {len(df)}\")\n",
        "    print(f\"English documents: {len(en_data)}\")\n",
        "    print(f\"Urdu documents: {len(ur_data)}\")\n",
        "\n",
        "    return df\n"
      ],
      "metadata": {
        "id": "d_7mSB5UB7fI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_text(df):\n",
        "    \"\"\"\n",
        "    Clean and preprocess the collected text while preserving Urdu characters\n",
        "    \"\"\"\n",
        "    print(\"\\n=== Step 4: Preprocessing Text ===\")\n",
        "\n",
        "    def clean_text(text):\n",
        "        # Keep Urdu characters and punctuation\n",
        "        text = re.sub(r'\\s+', ' ', text)\n",
        "        text = re.sub(r'[^\\w\\s.,!?؟،-]', '', text)  # Modified to keep Urdu punctuation\n",
        "        return text.strip()\n",
        "\n",
        "    df['cleaned_content'] = df['content'].apply(clean_text)\n",
        "    print(\"Sample of cleaned text (English):\")\n",
        "    print(df[df['language'] == 'en']['cleaned_content'].iloc[0][:200])\n",
        "    print(\"\\nSample of cleaned text (Urdu):\")\n",
        "    print(df[df['language'] == 'ur']['cleaned_content'].iloc[0][:200])\n",
        "\n",
        "    return df"
      ],
      "metadata": {
        "id": "sDj0wJivGwQD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def apply_chunking(df):\n",
        "    \"\"\"\n",
        "    Implement both fixed-length and sentence-based chunking strategies\n",
        "    \"\"\"\n",
        "    print(\"\\n=== Step 5: Applying Chunking Strategies ===\")\n",
        "\n",
        "    def create_fixed_length_chunks(text, chunk_size=500, overlap=50):\n",
        "        chunks = []\n",
        "        start = 0\n",
        "        text_length = len(text)\n",
        "\n",
        "        while start < text_length:\n",
        "            end = start + chunk_size\n",
        "            chunk = text[start:end]\n",
        "            chunks.append(chunk)\n",
        "            start = end - overlap\n",
        "\n",
        "        return chunks\n",
        "\n",
        "    def create_sentence_chunks(text, max_chunk_size=500):\n",
        "        # Simple sentence splitting for both English and Urdu\n",
        "        # For English, use NLTK's sent_tokenize\n",
        "        # For Urdu, split on common sentence endings\n",
        "        if any('\\u0600' <= char <= '\\u06FF' for char in text):  # Check if text contains Urdu characters\n",
        "            # Split Urdu text on common sentence endings\n",
        "            sentences = re.split(r'[.!?؟،]', text)\n",
        "            sentences = [s.strip() for s in sentences if s.strip()]\n",
        "        else:\n",
        "            # Use NLTK for English text\n",
        "            try:\n",
        "                sentences = sent_tokenize(text)\n",
        "            except:\n",
        "                # Fallback to simple splitting if NLTK fails\n",
        "                sentences = re.split(r'[.!?]', text)\n",
        "                sentences = [s.strip() for s in sentences if s.strip()]\n",
        "\n",
        "        chunks = []\n",
        "        current_chunk = []\n",
        "        current_size = 0\n",
        "\n",
        "        for sentence in sentences:\n",
        "            sentence_size = len(sentence)\n",
        "            if current_size + sentence_size > max_chunk_size and current_chunk:\n",
        "                chunks.append(' '.join(current_chunk))\n",
        "                current_chunk = [sentence]\n",
        "                current_size = sentence_size\n",
        "            else:\n",
        "                current_chunk.append(sentence)\n",
        "                current_size += sentence_size\n",
        "\n",
        "        if current_chunk:\n",
        "            chunks.append(' '.join(current_chunk))\n",
        "\n",
        "        return chunks\n",
        "\n",
        "    # Apply both chunking strategies\n",
        "    df['fixed_chunks'] = df['cleaned_content'].apply(lambda x: create_fixed_length_chunks(x))\n",
        "    df['sentence_chunks'] = df['cleaned_content'].apply(lambda x: create_sentence_chunks(x))\n",
        "\n",
        "    print(\"Sample chunks from both strategies (English):\")\n",
        "    print(\"\\nFixed-length chunks:\")\n",
        "    print(df[df['language'] == 'en']['fixed_chunks'].iloc[0][0])\n",
        "    print(\"\\nSentence-based chunks:\")\n",
        "    print(df[df['language'] == 'en']['sentence_chunks'].iloc[0][0])\n",
        "\n",
        "    return df"
      ],
      "metadata": {
        "id": "a56dV7k0DbJE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_embeddings(df):\n",
        "    \"\"\"\n",
        "    Generate embeddings using multilingual models that support both English and Urdu\n",
        "    \"\"\"\n",
        "    print(\"\\n=== Step 6: Generating Embeddings ===\")\n",
        "\n",
        "    def get_embeddings(texts, model_name, batch_size=32):\n",
        "        print(f\"Loading model: {model_name}\")\n",
        "        model = SentenceTransformer(model_name)\n",
        "\n",
        "        # Process in batches\n",
        "        all_embeddings = []\n",
        "        for i in tqdm(range(0, len(texts), batch_size), desc=f\"Generating embeddings with {model_name}\"):\n",
        "            batch_texts = texts[i:i + batch_size]\n",
        "            batch_embeddings = model.encode(batch_texts, show_progress_bar=False)\n",
        "            all_embeddings.extend(batch_embeddings)\n",
        "\n",
        "        return np.array(all_embeddings)\n",
        "\n",
        "    # Using a single best model for both languages\n",
        "    model_name = 'sentence-transformers/LaBSE'  # Language-agnostic BERT Sentence Embedding\n",
        "\n",
        "    # Process only sentence chunks as they're more meaningful\n",
        "    print(\"Processing sentence chunks...\")\n",
        "    all_chunks = []\n",
        "    for chunks in df['sentence_chunks']:\n",
        "        all_chunks.extend(chunks)\n",
        "\n",
        "    print(f\"Total chunks to process: {len(all_chunks)}\")\n",
        "\n",
        "    # Generate embeddings\n",
        "    embeddings = get_embeddings(all_chunks, model_name)\n",
        "\n",
        "    # Save embeddings\n",
        "    embeddings_dict = {'sentence_chunks_LaBSE': embeddings}\n",
        "    np.save('embeddings_sentence_chunks_LaBSE.npy', embeddings)\n",
        "\n",
        "    print(\"Embeddings generation completed!\")\n",
        "    return embeddings_dict\n"
      ],
      "metadata": {
        "id": "Ip0_v0G9Dmuc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install --upgrade pinecone-client"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VhmAm6wTR-np",
        "outputId": "f01a7561-a01f-42b0-f838-51b6b5a4f441"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pinecone-client\n",
            "  Using cached pinecone_client-6.0.0-py3-none-any.whl.metadata (3.4 kB)\n",
            "Requirement already satisfied: certifi>=2019.11.17 in /usr/local/lib/python3.11/dist-packages (from pinecone-client) (2025.4.26)\n",
            "Requirement already satisfied: pinecone-plugin-interface<0.0.8,>=0.0.7 in /usr/local/lib/python3.11/dist-packages (from pinecone-client) (0.0.7)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.11/dist-packages (from pinecone-client) (2.9.0.post0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.11/dist-packages (from pinecone-client) (4.13.2)\n",
            "Requirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from pinecone-client) (2.4.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.5.3->pinecone-client) (1.17.0)\n",
            "Using cached pinecone_client-6.0.0-py3-none-any.whl (6.7 kB)\n",
            "Installing collected packages: pinecone-client\n",
            "Successfully installed pinecone-client-6.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def setup_pinecone():\n",
        "    \"\"\"\n",
        "    Connect to the existing Pinecone index 'urdu-eng-rag' in AWS us-east-1 with dimension 768.\n",
        "    \"\"\"\n",
        "    print(\"\\n=== Step 7: Setting up Pinecone ===\")\n",
        "    try:\n",
        "        PINECONE_API_KEY = \"pcsk_ffw9K_3f5Semen1JhXkdPBYdJsZEWmZfCP7T1tsRixFSHb1GGuWiL7ij21K24rsEhTbBo\"\n",
        "        index_name = \"urdu-eng-rag\"\n",
        "        dimension = 768  # Must match your index\n",
        "\n",
        "        pc = Pinecone(api_key=PINECONE_API_KEY)\n",
        "\n",
        "        # Only create if not exists (should already exist as per your screenshot)\n",
        "        if index_name not in pc.list_indexes().names():\n",
        "            print(f\"Creating new index: {index_name}\")\n",
        "            pc.create_index(\n",
        "                name=index_name,\n",
        "                dimension=dimension,\n",
        "                metric='cosine',\n",
        "                spec=ServerlessSpec(\n",
        "                    cloud='aws',\n",
        "                    region='us-east-1'\n",
        "                )\n",
        "            )\n",
        "            print(\"Index created successfully!\")\n",
        "        else:\n",
        "            print(f\"Using existing index: {index_name}\")\n",
        "\n",
        "        index = pc.Index(index_name)\n",
        "        print(\"Pinecone setup completed successfully!\")\n",
        "        return index\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error setting up Pinecone: {str(e)}\")\n",
        "        raise"
      ],
      "metadata": {
        "id": "6jOpHolUDuvG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def store_embeddings(index, df, embeddings_dict):\n",
        "    \"\"\"\n",
        "    Store the generated embeddings in Pinecone with appropriate metadata\n",
        "    \"\"\"\n",
        "    print(\"\\n=== Step 8: Storing Embeddings ===\")\n",
        "\n",
        "    def store_embeddings_batch(embeddings, chunks, metadata, batch_size=100):\n",
        "        for i in range(0, len(embeddings), batch_size):\n",
        "            batch_embeddings = embeddings[i:i + batch_size]\n",
        "            batch_chunks = chunks[i:i + batch_size]\n",
        "\n",
        "            vectors = []\n",
        "            for j, (embedding, chunk) in enumerate(zip(batch_embeddings, batch_chunks)):\n",
        "                vector = {\n",
        "                    'id': f'vec_{i+j}',\n",
        "                    'values': embedding.tolist(),\n",
        "                    'metadata': {\n",
        "                        'text': chunk,\n",
        "                        **metadata\n",
        "                    }\n",
        "                }\n",
        "                vectors.append(vector)\n",
        "\n",
        "            index.upsert(vectors=vectors)\n",
        "\n",
        "    # Get the embeddings and chunks\n",
        "    embeddings = embeddings_dict['sentence_chunks_LaBSE']\n",
        "    all_chunks = []\n",
        "    for chunks in df['sentence_chunks']:\n",
        "        all_chunks.extend(chunks)\n",
        "\n",
        "    print(f\"Storing {len(all_chunks)} chunks with their embeddings...\")\n",
        "\n",
        "    # Store embeddings with metadata\n",
        "    metadata = {\n",
        "        'chunking_strategy': 'sentence_chunks',\n",
        "        'model': 'LaBSE'\n",
        "    }\n",
        "\n",
        "    store_embeddings_batch(embeddings, all_chunks, metadata)\n",
        "    print(\"Embeddings stored successfully!\")"
      ],
      "metadata": {
        "id": "YGjERmBaFpTa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def query_pinecone(index, query_text, model_name='sentence-transformers/LaBSE', top_k=5):\n",
        "    model = SentenceTransformer(model_name)\n",
        "    query_embedding = model.encode(query_text)\n",
        "    results = index.query(\n",
        "        vector=query_embedding.tolist(),\n",
        "        top_k=top_k,\n",
        "        include_metadata=True\n",
        "    )\n",
        "    return results"
      ],
      "metadata": {
        "id": "g2av6KMgYcwh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_rag(index):\n",
        "    \"\"\"\n",
        "    Test the chatbot with queries in both English and Urdu\n",
        "    \"\"\"\n",
        "    print(\"\\n=== Step 9: Testing RAG System ===\")\n",
        "\n",
        "    test_queries = [\n",
        "        \"What are the health benefits of Pakistani cuisine?\",\n",
        "        \"پاکستانی کھانوں کے صحت کے فوائد کیا ہیں؟\",\n",
        "        \"How to maintain a balanced diet with traditional Pakistani food?\",\n",
        "        \"روایتی پاکستانی کھانوں کے ساتھ متوازن غذا کیسے برقرار رکھیں؟\"\n",
        "    ]\n",
        "\n",
        "    for query in test_queries:\n",
        "        print(f\"\\nQuery: {query}\")\n",
        "        results = query_pinecone(index, query)\n",
        "\n",
        "        print(\"\\nTop results:\")\n",
        "        for match in results.matches:\n",
        "            print(f\"\\nScore: {match.score}\")\n",
        "            print(f\"Text: {match.metadata['text'][:200]}...\")\n",
        "            print(f\"Chunking Strategy: {match.metadata['chunking_strategy']}\")\n",
        "            print(f\"Model: {match.metadata['model']}\")"
      ],
      "metadata": {
        "id": "BNOeNk8zF3rh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def analyze_performance(index):\n",
        "    \"\"\"\n",
        "    Analyze the performance of different models and chunking strategies\n",
        "    \"\"\"\n",
        "    print(\"\\n=== Step 10: Analyzing Performance ===\")\n",
        "\n",
        "    def evaluate_combinations(queries):\n",
        "        results = []\n",
        "        models = [\n",
        "            'sentence-transformers/LaBSE'\n",
        "        ]\n",
        "        chunking_strategies = ['sentence_chunks']\n",
        "        for query in queries:\n",
        "            for model in models:\n",
        "                for strategy in chunking_strategies:\n",
        "                    results_query = query_pinecone(index, query, model)\n",
        "                    avg_score = sum(match.score for match in results_query.matches) / len(results_query.matches)\n",
        "                    results.append({\n",
        "                        'query': query,\n",
        "                        'model': model.split('/')[-1],\n",
        "                        'strategy': strategy,\n",
        "                        'avg_score': avg_score\n",
        "                    })\n",
        "        return pd.DataFrame(results)\n",
        "\n",
        "    test_queries = [\n",
        "        \"What are the health benefits of Pakistani cuisine?\",\n",
        "        \"پاکستانی کھانوں کے صحت کے فوائد کیا ہیں؟\"\n",
        "    ]\n",
        "\n",
        "    results_df = evaluate_combinations(test_queries)\n",
        "\n",
        "    # Create pivot table for visualization\n",
        "    pivot_df = results_df.pivot_table(\n",
        "        values='avg_score',\n",
        "        index='model',\n",
        "        columns='strategy'\n",
        "    )\n",
        "\n",
        "    # Plot results\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    pivot_df.plot(kind='bar')\n",
        "    plt.title('Model-Chunking Strategy Performance Comparison')\n",
        "    plt.xlabel('Model')\n",
        "    plt.ylabel('Average Score')\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('performance_comparison.png')\n",
        "    plt.close()\n",
        "\n",
        "    # Print detailed results\n",
        "    print(\"\\nDetailed Results:\")\n",
        "    print(results_df.sort_values('avg_score', ascending=False))\n",
        "\n",
        "    return results_df"
      ],
      "metadata": {
        "id": "IQ9S9wQBGAc_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    try:\n",
        "        # Step 1: Collect data\n",
        "        df = collect_data()\n",
        "\n",
        "        # Step 2: Preprocess text\n",
        "        df = preprocess_text(df)\n",
        "\n",
        "        # Step 3: Apply chunking\n",
        "        df = apply_chunking(df)\n",
        "\n",
        "        # Step 4: Generate embeddings\n",
        "        embeddings_dict = generate_embeddings(df)\n",
        "\n",
        "        # Step 5: Setup Pinecone\n",
        "        index = setup_pinecone()\n",
        "\n",
        "        # Step 6: Store embeddings\n",
        "        store_embeddings(index, df, embeddings_dict)\n",
        "\n",
        "        # Step 7: Test RAG\n",
        "        test_rag(index)\n",
        "\n",
        "        # Step 8: Analyze performance\n",
        "        results_df = analyze_performance(index)\n",
        "\n",
        "        print(\"\\nAll steps completed successfully!\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {str(e)}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "H9GU2-tIGHsZ",
        "outputId": "e80f6452-78d9-4adc-9235-031449db1567"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Step 3: Collecting Data ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Collecting English articles: 100%|██████████| 7/7 [00:02<00:00,  2.63it/s]\n",
            "Collecting Urdu articles:  50%|█████     | 1/2 [00:00<00:00,  2.56it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: No content found for Pakistani_cuisine in ur\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Collecting Urdu articles: 100%|██████████| 2/2 [00:00<00:00,  2.55it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: No content found for Indian_cuisine in ur\n",
            "\n",
            "Data Collection Summary:\n",
            "Total documents collected: 12\n",
            "English documents: 6\n",
            "Urdu documents: 6\n",
            "\n",
            "=== Step 4: Preprocessing Text ===\n",
            "Sample of cleaned text (English):\n",
            "Nutrition is the biochemical and physiological process by which an organism uses food and water to support its life. The intake of these substances provides organisms with nutrients divided into macro\n",
            "\n",
            "Sample of cleaned text (Urdu):\n",
            "پاکستانی کھانوں میں بریانی، کڑاہی، نہاری، چپلی کباب، سموسے، پکوڑے، حلیم، اور مختلف قسم کی روٹیاں شامل ہیں یہ کھانے صحت بخش اجزاء سے بنائے جاتے ہیں اور متوازن غذا کا حصہ ہیں\n",
            "\n",
            "=== Step 5: Applying Chunking Strategies ===\n",
            "Sample chunks from both strategies (English):\n",
            "\n",
            "Fixed-length chunks:\n",
            "Nutrition is the biochemical and physiological process by which an organism uses food and water to support its life. The intake of these substances provides organisms with nutrients divided into macro- and micro- which can be metabolized to create energy and chemical structures too much or too little of an essential nutrient can cause malnutrition. Nutritional science, the study of nutrition as a hard science, typically emphasizes human nutrition. The type of organism determines what nutrients i\n",
            "\n",
            "Sentence-based chunks:\n",
            "Nutrition is the biochemical and physiological process by which an organism uses food and water to support its life The intake of these substances provides organisms with nutrients divided into macro- and micro- which can be metabolized to create energy and chemical structures too much or too little of an essential nutrient can cause malnutrition Nutritional science, the study of nutrition as a hard science, typically emphasizes human nutrition\n",
            "\n",
            "=== Step 6: Generating Embeddings ===\n",
            "Processing sentence chunks...\n",
            "Total chunks to process: 28\n",
            "Loading model: sentence-transformers/LaBSE\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Generating embeddings with sentence-transformers/LaBSE: 100%|██████████| 1/1 [00:07<00:00,  7.26s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embeddings generation completed!\n",
            "\n",
            "=== Step 7: Setting up Pinecone ===\n",
            "Using existing index: urdu-eng-rag\n",
            "Pinecone setup completed successfully!\n",
            "\n",
            "=== Step 8: Storing Embeddings ===\n",
            "Storing 28 chunks with their embeddings...\n",
            "Embeddings stored successfully!\n",
            "\n",
            "=== Step 9: Testing RAG System ===\n",
            "\n",
            "Query: What are the health benefits of Pakistani cuisine?\n",
            "\n",
            "Top results:\n",
            "\n",
            "Score: 0.591887236\n",
            "Text: پاکستانی کھانوں میں موجود اجزاء صحت کے لیے بہت فائدہ مند ہیں دالیں پروٹین کا اچھا ذریعہ ہیں سبزیاں وٹامنز اور معدنیات سے بھرپور ہیں اور گوشت آئرن اور پروٹین فراہم کرتا ہے...\n",
            "Chunking Strategy: sentence_chunks\n",
            "Model: LaBSE\n",
            "\n",
            "Score: 0.572941363\n",
            "Text: پاکستانی کھانوں میں بریانی کڑاہی نہاری چپلی کباب سموسے پکوڑے حلیم اور مختلف قسم کی روٹیاں شامل ہیں یہ کھانے صحت بخش اجزاء سے بنائے جاتے ہیں اور متوازن غذا کا حصہ ہیں...\n",
            "Chunking Strategy: sentence_chunks\n",
            "Model: LaBSE\n",
            "\n",
            "Score: 0.55020833\n",
            "Text: روایتی پاکستانی غذا میں چاول روٹی دالیں سبزیاں اور گوشت شامل ہیں یہ غذا توانائی اور غذائیت سے بھرپور ہوتی ہے...\n",
            "Chunking Strategy: sentence_chunks\n",
            "Model: LaBSE\n",
            "\n",
            "Score: 0.548680365\n",
            "Text: پاکستانی کھانوں میں مختلف طریقے استعمال ہوتے ہیں جیسے بھوننا پکانا اور تڑکا لگانا یہ طریقے کھانوں کی غذائیت کو برقرار رکھتے ہیں اور ذائقہ بڑھاتے ہیں...\n",
            "Chunking Strategy: sentence_chunks\n",
            "Model: LaBSE\n",
            "\n",
            "Score: 0.503115475\n",
            "Text: Pakistani cuisine Urdu پاکستانی پکوان, romanized pākistānī pakwān is a blend of regional cooking styles and flavours from across South, Central and West Asia It is a culmination of Iranic, Indic  Arab...\n",
            "Chunking Strategy: sentence_chunks\n",
            "Model: LaBSE\n",
            "\n",
            "Query: پاکستانی کھانوں کے صحت کے فوائد کیا ہیں؟\n",
            "\n",
            "Top results:\n",
            "\n",
            "Score: 0.666916668\n",
            "Text: پاکستانی کھانوں میں موجود اجزاء صحت کے لیے بہت فائدہ مند ہیں دالیں پروٹین کا اچھا ذریعہ ہیں سبزیاں وٹامنز اور معدنیات سے بھرپور ہیں اور گوشت آئرن اور پروٹین فراہم کرتا ہے...\n",
            "Chunking Strategy: sentence_chunks\n",
            "Model: LaBSE\n",
            "\n",
            "Score: 0.614987433\n",
            "Text: روایتی پاکستانی غذا میں چاول روٹی دالیں سبزیاں اور گوشت شامل ہیں یہ غذا توانائی اور غذائیت سے بھرپور ہوتی ہے...\n",
            "Chunking Strategy: sentence_chunks\n",
            "Model: LaBSE\n",
            "\n",
            "Score: 0.603779078\n",
            "Text: پاکستانی کھانوں میں مختلف طریقے استعمال ہوتے ہیں جیسے بھوننا پکانا اور تڑکا لگانا یہ طریقے کھانوں کی غذائیت کو برقرار رکھتے ہیں اور ذائقہ بڑھاتے ہیں...\n",
            "Chunking Strategy: sentence_chunks\n",
            "Model: LaBSE\n",
            "\n",
            "Score: 0.598625183\n",
            "Text: پاکستانی کھانوں میں بریانی کڑاہی نہاری چپلی کباب سموسے پکوڑے حلیم اور مختلف قسم کی روٹیاں شامل ہیں یہ کھانے صحت بخش اجزاء سے بنائے جاتے ہیں اور متوازن غذا کا حصہ ہیں...\n",
            "Chunking Strategy: sentence_chunks\n",
            "Model: LaBSE\n",
            "\n",
            "Score: 0.513296425\n",
            "Text: صحت مند کھانے میں تازہ سبزیاں پھل دالیں اور کم چکنائی والی پروٹین شامل ہونی چاہئیں روزانہ کم از کم 8 گلاس پانی پینا چاہیے اور پروسیسڈ فوڈز سے پرہیز کرنا چاہیے...\n",
            "Chunking Strategy: sentence_chunks\n",
            "Model: LaBSE\n",
            "\n",
            "Query: How to maintain a balanced diet with traditional Pakistani food?\n",
            "\n",
            "Top results:\n",
            "\n",
            "Score: 0.56971246\n",
            "Text: روایتی پاکستانی غذا میں چاول روٹی دالیں سبزیاں اور گوشت شامل ہیں یہ غذا توانائی اور غذائیت سے بھرپور ہوتی ہے...\n",
            "Chunking Strategy: sentence_chunks\n",
            "Model: LaBSE\n",
            "\n",
            "Score: 0.538518906\n",
            "Text: پاکستانی کھانوں میں مختلف طریقے استعمال ہوتے ہیں جیسے بھوننا پکانا اور تڑکا لگانا یہ طریقے کھانوں کی غذائیت کو برقرار رکھتے ہیں اور ذائقہ بڑھاتے ہیں...\n",
            "Chunking Strategy: sentence_chunks\n",
            "Model: LaBSE\n",
            "\n",
            "Score: 0.506372\n",
            "Text: پاکستانی کھانوں میں بریانی کڑاہی نہاری چپلی کباب سموسے پکوڑے حلیم اور مختلف قسم کی روٹیاں شامل ہیں یہ کھانے صحت بخش اجزاء سے بنائے جاتے ہیں اور متوازن غذا کا حصہ ہیں...\n",
            "Chunking Strategy: sentence_chunks\n",
            "Model: LaBSE\n",
            "\n",
            "Score: 0.483576298\n",
            "Text: Pakistani cuisine Urdu پاکستانی پکوان, romanized pākistānī pakwān is a blend of regional cooking styles and flavours from across South, Central and West Asia It is a culmination of Iranic, Indic  Arab...\n",
            "Chunking Strategy: sentence_chunks\n",
            "Model: LaBSE\n",
            "\n",
            "Score: 0.471767426\n",
            "Text: متوازن غذا میں کاربوہائیڈریٹس پروٹین چکنائی وٹامنز اور معدنیات کی مناسب مقدار ہونی چاہیے پاکستانی کھانوں میں یہ تمام اجزاء موجود ہیں لیکن اعتدال میں کھانا ضروری ہے...\n",
            "Chunking Strategy: sentence_chunks\n",
            "Model: LaBSE\n",
            "\n",
            "Query: روایتی پاکستانی کھانوں کے ساتھ متوازن غذا کیسے برقرار رکھیں؟\n",
            "\n",
            "Top results:\n",
            "\n",
            "Score: 0.651952267\n",
            "Text: روایتی پاکستانی غذا میں چاول روٹی دالیں سبزیاں اور گوشت شامل ہیں یہ غذا توانائی اور غذائیت سے بھرپور ہوتی ہے...\n",
            "Chunking Strategy: sentence_chunks\n",
            "Model: LaBSE\n",
            "\n",
            "Score: 0.628228366\n",
            "Text: پاکستانی کھانوں میں مختلف طریقے استعمال ہوتے ہیں جیسے بھوننا پکانا اور تڑکا لگانا یہ طریقے کھانوں کی غذائیت کو برقرار رکھتے ہیں اور ذائقہ بڑھاتے ہیں...\n",
            "Chunking Strategy: sentence_chunks\n",
            "Model: LaBSE\n",
            "\n",
            "Score: 0.560449302\n",
            "Text: پاکستانی کھانوں میں بریانی کڑاہی نہاری چپلی کباب سموسے پکوڑے حلیم اور مختلف قسم کی روٹیاں شامل ہیں یہ کھانے صحت بخش اجزاء سے بنائے جاتے ہیں اور متوازن غذا کا حصہ ہیں...\n",
            "Chunking Strategy: sentence_chunks\n",
            "Model: LaBSE\n",
            "\n",
            "Score: 0.527088881\n",
            "Text: پاکستانی کھانوں میں موجود اجزاء صحت کے لیے بہت فائدہ مند ہیں دالیں پروٹین کا اچھا ذریعہ ہیں سبزیاں وٹامنز اور معدنیات سے بھرپور ہیں اور گوشت آئرن اور پروٹین فراہم کرتا ہے...\n",
            "Chunking Strategy: sentence_chunks\n",
            "Model: LaBSE\n",
            "\n",
            "Score: 0.518896282\n",
            "Text: متوازن غذا میں کاربوہائیڈریٹس پروٹین چکنائی وٹامنز اور معدنیات کی مناسب مقدار ہونی چاہیے پاکستانی کھانوں میں یہ تمام اجزاء موجود ہیں لیکن اعتدال میں کھانا ضروری ہے...\n",
            "Chunking Strategy: sentence_chunks\n",
            "Model: LaBSE\n",
            "\n",
            "=== Step 10: Analyzing Performance ===\n",
            "\n",
            "Detailed Results:\n",
            "                                               query  model         strategy  \\\n",
            "1           پاکستانی کھانوں کے صحت کے فوائد کیا ہیں؟  LaBSE  sentence_chunks   \n",
            "0  What are the health benefits of Pakistani cuis...  LaBSE  sentence_chunks   \n",
            "\n",
            "   avg_score  \n",
            "1   0.599521  \n",
            "0   0.553470  \n",
            "\n",
            "All steps completed successfully!\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x600 with 0 Axes>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Assignment 3"
      ],
      "metadata": {
        "id": "aBTiw-ePaaX5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install groq"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "adeJ6EudJg2Q",
        "outputId": "6e588ac2-4e50-458a-d712-6dfadb6c2b93"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: groq in /usr/local/lib/python3.11/dist-packages (0.26.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from groq) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from groq) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from groq) (0.28.1)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from groq) (2.11.4)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from groq) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.10 in /usr/local/lib/python3.11/dist-packages (from groq) (4.13.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->groq) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->groq) (2025.4.26)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->groq) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->groq) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->groq) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->groq) (0.4.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from groq import Groq"
      ],
      "metadata": {
        "id": "Tviq5dwnJlFc"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os"
      ],
      "metadata": {
        "id": "E7g38fy7xbpb"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set Groq API key as environment variable\n",
        "os.environ[\"GROQ_API_KEY\"] = \"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\"\n",
        "\n",
        "# Initialize Groq client\n",
        "client = Groq()\n",
        "\n",
        "# List available models\n",
        "try:\n",
        "    models = client.models.list()\n",
        "    print(\"Available models:\")\n",
        "    print(models)  # This will print the raw response\n",
        "except Exception as e:\n",
        "    print(f\"Error listing models: {str(e)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k4B9YKkZOoo7",
        "outputId": "d5372bac-47f0-457d-c743-160174adf4f6"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Available models:\n",
            "ModelListResponse(data=[Model(id='compound-beta-mini', created=1742953279, object='model', owned_by='Groq', active=True, context_window=8192, public_apps=None, max_completion_tokens=8192), Model(id='playai-tts-arabic', created=1740682783, object='model', owned_by='PlayAI', active=True, context_window=8192, public_apps=None, max_completion_tokens=8192), Model(id='qwen-qwq-32b', created=1741214760, object='model', owned_by='Alibaba Cloud', active=True, context_window=131072, public_apps=None, max_completion_tokens=131072), Model(id='llama-3.1-8b-instant', created=1693721698, object='model', owned_by='Meta', active=True, context_window=131072, public_apps=None, max_completion_tokens=131072), Model(id='compound-beta', created=1740880017, object='model', owned_by='Groq', active=True, context_window=8192, public_apps=None, max_completion_tokens=8192), Model(id='llama3-70b-8192', created=1693721698, object='model', owned_by='Meta', active=True, context_window=8192, public_apps=None, max_completion_tokens=8192), Model(id='llama3-8b-8192', created=1693721698, object='model', owned_by='Meta', active=True, context_window=8192, public_apps=None, max_completion_tokens=8192), Model(id='meta-llama/llama-4-maverick-17b-128e-instruct', created=1743877158, object='model', owned_by='Meta', active=True, context_window=131072, public_apps=None, max_completion_tokens=8192), Model(id='whisper-large-v3-turbo', created=1728413088, object='model', owned_by='OpenAI', active=True, context_window=448, public_apps=None, max_completion_tokens=448), Model(id='whisper-large-v3', created=1693721698, object='model', owned_by='OpenAI', active=True, context_window=448, public_apps=None, max_completion_tokens=448), Model(id='llama-guard-3-8b', created=1693721698, object='model', owned_by='Meta', active=True, context_window=8192, public_apps=None, max_completion_tokens=8192), Model(id='llama-3.3-70b-versatile', created=1733447754, object='model', owned_by='Meta', active=True, context_window=131072, public_apps=None, max_completion_tokens=32768), Model(id='deepseek-r1-distill-llama-70b', created=1737924940, object='model', owned_by='DeepSeek / Meta', active=True, context_window=131072, public_apps=None, max_completion_tokens=131072), Model(id='meta-llama/llama-prompt-guard-2-86m', created=1748632165, object='model', owned_by='Meta', active=True, context_window=512, public_apps=None, max_completion_tokens=512), Model(id='playai-tts', created=1740682771, object='model', owned_by='PlayAI', active=True, context_window=8192, public_apps=None, max_completion_tokens=8192), Model(id='distil-whisper-large-v3-en', created=1693721698, object='model', owned_by='Hugging Face', active=True, context_window=448, public_apps=None, max_completion_tokens=448), Model(id='meta-llama/llama-guard-4-12b', created=1746743847, object='model', owned_by='Meta', active=True, context_window=131072, public_apps=None, max_completion_tokens=1024), Model(id='mistral-saba-24b', created=1739996492, object='model', owned_by='Mistral AI', active=True, context_window=32768, public_apps=None, max_completion_tokens=32768), Model(id='meta-llama/llama-prompt-guard-2-22m', created=1748632101, object='model', owned_by='Meta', active=True, context_window=512, public_apps=None, max_completion_tokens=512), Model(id='allam-2-7b', created=1737672203, object='model', owned_by='SDAIA', active=True, context_window=4096, public_apps=None, max_completion_tokens=4096), Model(id='meta-llama/llama-4-scout-17b-16e-instruct', created=1743874824, object='model', owned_by='Meta', active=True, context_window=131072, public_apps=None, max_completion_tokens=8192), Model(id='gemma2-9b-it', created=1693721698, object='model', owned_by='Google', active=True, context_window=8192, public_apps=None, max_completion_tokens=8192)], object='list')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set Groq API key as environment variable\n",
        "os.environ[\"GROQ_API_KEY\"] = \"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\"\n",
        "\n",
        "# Initialize Groq client\n",
        "client = Groq()\n",
        "\n",
        "# Test the connection\n",
        "try:\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"llama-3.3-70b-versatile\",  # Updated model name\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "            {\"role\": \"user\", \"content\": \"Hello, are you working?\"}\n",
        "        ],\n",
        "        max_tokens=50,\n",
        "        temperature=0.3\n",
        "    )\n",
        "    print(\"Groq connection successful!\")\n",
        "except Exception as e:\n",
        "    print(f\"Error connecting to Groq: {str(e)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8YrtW__CPC6T",
        "outputId": "375c95ff-2555-4566-997d-515f3edbf986"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Groq connection successful!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def collect_multilingual_data():\n",
        "    \"\"\"\n",
        "    Enhanced data collection from multiple sources in English and Urdu\n",
        "    \"\"\"\n",
        "    print(\"\\n=== Step 1: Collecting Multilingual Data ===\")\n",
        "\n",
        "    # English sources\n",
        "    en_sources = {\n",
        "        'wikipedia': [\n",
        "            'Nutrition',\n",
        "            'Healthy_diet',\n",
        "            'Pakistani_cuisine',\n",
        "            'Indian_cuisine',\n",
        "            'South_Asian_cuisine',\n",
        "            'Halal',\n",
        "            'Desi_cuisine',\n",
        "            'Traditional_medicine',\n",
        "            'Herbal_medicine',\n",
        "            'Ayurveda'\n",
        "        ],\n",
        "        'custom': [\n",
        "            {\n",
        "                'title': 'Traditional Pakistani Cooking Methods',\n",
        "                'content': \"\"\"\n",
        "                Traditional Pakistani cooking methods include:\n",
        "                1. Dum cooking - slow cooking in sealed containers\n",
        "                2. Tandoori cooking - clay oven cooking\n",
        "                3. Bhunna - sautéing and browning\n",
        "                4. Dhuanaar - smoking with charcoal\n",
        "                These methods preserve nutrients and enhance flavors.\n",
        "                \"\"\"\n",
        "            },\n",
        "            {\n",
        "                'title': 'Health Benefits of Spices',\n",
        "                'content': \"\"\"\n",
        "                Common Pakistani spices and their health benefits:\n",
        "                - Turmeric: Anti-inflammatory properties\n",
        "                - Cumin: Aids digestion\n",
        "                - Coriander: Rich in antioxidants\n",
        "                - Cardamom: Helps with respiratory issues\n",
        "                - Cinnamon: Regulates blood sugar\n",
        "                \"\"\"\n",
        "            }\n",
        "        ]\n",
        "    }\n",
        "\n",
        "    # Urdu sources\n",
        "    ur_sources = {\n",
        "        'wikipedia': [\n",
        "            'Pakistani_cuisine',\n",
        "            'Indian_cuisine',\n",
        "            'Traditional_medicine'\n",
        "        ],\n",
        "        'custom': [\n",
        "            {\n",
        "                'title': 'پاکستانی کھانوں کی صحت بخش خصوصیات',\n",
        "                'content': \"\"\"\n",
        "                پاکستانی کھانوں میں استعمال ہونے والے مصالحے صحت کے لیے بہت فائدہ مند ہیں۔\n",
        "                ہلدی میں اینٹی انفلیمیٹری خصوصیات ہیں، زیرہ ہاضمہ کو بہتر بناتا ہے،\n",
        "                دھنیا اینٹی آکسیڈنٹس سے بھرپور ہے، الائچی سانس کی بیماریوں میں مدد کرتی ہے،\n",
        "                اور دارچینی خون میں شکر کی سطح کو کنٹرول کرتی ہے۔\n",
        "                \"\"\"\n",
        "            },\n",
        "            {\n",
        "                'title': 'روایتی پاکستانی کھانوں کے طریقے',\n",
        "                'content': \"\"\"\n",
        "                روایتی پاکستانی کھانوں کے طریقے:\n",
        "                1. دم پخت - مہر بند برتنوں میں آہستہ پکانا\n",
        "                2. تندوری - مٹی کے تندور میں پکانا\n",
        "                3. بھوننا - ہلکی آنچ پر پکانا\n",
        "                4. دھونیار - کوئلے کی دھونی دینا\n",
        "                یہ طریقے غذائیت کو برقرار رکھتے ہیں اور ذائقہ بڑھاتے ہیں۔\n",
        "                \"\"\"\n",
        "            }\n",
        "        ]\n",
        "    }\n",
        "\n",
        "    def get_wikipedia_content(title, lang='en', max_retries=3, delay=2):\n",
        "        for attempt in range(max_retries):\n",
        "            try:\n",
        "                url = f'https://{lang}.wikipedia.org/w/api.php'\n",
        "                params = {\n",
        "                    'action': 'query',\n",
        "                    'format': 'json',\n",
        "                    'titles': title,\n",
        "                    'prop': 'extracts',\n",
        "                    'exintro': True,\n",
        "                    'explaintext': True\n",
        "                }\n",
        "                headers = {\n",
        "                    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'\n",
        "                }\n",
        "                response = requests.get(url, params=params, headers=headers)\n",
        "                response.raise_for_status()\n",
        "\n",
        "                data = response.json()\n",
        "                pages = data['query']['pages']\n",
        "                page = next(iter(pages.values()))\n",
        "\n",
        "                if 'extract' not in page:\n",
        "                    print(f\"Warning: No content found for {title} in {lang}\")\n",
        "                    return None\n",
        "\n",
        "                return page['extract']\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error fetching {title} in {lang}: {str(e)}\")\n",
        "                if attempt < max_retries - 1:\n",
        "                    time.sleep(delay)\n",
        "                    continue\n",
        "                return None\n",
        "\n",
        "    # Collect data from all sources\n",
        "    all_data = []\n",
        "\n",
        "    # Collect English Wikipedia articles\n",
        "    for article in tqdm(en_sources['wikipedia'], desc='Collecting English Wikipedia articles'):\n",
        "        content = get_wikipedia_content(article, 'en')\n",
        "        if content:\n",
        "            all_data.append({\n",
        "                'title': article,\n",
        "                'content': content,\n",
        "                'language': 'en',\n",
        "                'source': 'wikipedia'\n",
        "            })\n",
        "\n",
        "    # Add English custom content\n",
        "    for item in en_sources['custom']:\n",
        "        all_data.append({\n",
        "            'title': item['title'],\n",
        "            'content': item['content'],\n",
        "            'language': 'en',\n",
        "            'source': 'custom'\n",
        "        })\n",
        "\n",
        "    # Collect Urdu Wikipedia articles\n",
        "    for article in tqdm(ur_sources['wikipedia'], desc='Collecting Urdu Wikipedia articles'):\n",
        "        content = get_wikipedia_content(article, 'ur')\n",
        "        if content:\n",
        "            all_data.append({\n",
        "                'title': article,\n",
        "                'content': content,\n",
        "                'language': 'ur',\n",
        "                'source': 'wikipedia'\n",
        "            })\n",
        "\n",
        "    # Add Urdu custom content\n",
        "    for item in ur_sources['custom']:\n",
        "        all_data.append({\n",
        "            'title': item['title'],\n",
        "            'content': item['content'],\n",
        "            'language': 'ur',\n",
        "            'source': 'custom'\n",
        "        })\n",
        "\n",
        "    # Convert to DataFrame\n",
        "    df = pd.DataFrame(all_data)\n",
        "\n",
        "    print(\"\\nData Collection Summary:\")\n",
        "    print(f\"Total documents: {len(df)}\")\n",
        "    print(f\"English documents: {len(df[df['language'] == 'en'])}\")\n",
        "    print(f\"Urdu documents: {len(df[df['language'] == 'ur'])}\")\n",
        "    print(f\"Wikipedia sources: {len(df[df['source'] == 'wikipedia'])}\")\n",
        "    print(f\"Custom sources: {len(df[df['source'] == 'custom'])}\")\n",
        "\n",
        "    return df"
      ],
      "metadata": {
        "id": "1bt1noBsdvLs"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_text(df):\n",
        "    \"\"\"\n",
        "    Clean and preprocess the collected text while preserving Urdu characters\n",
        "    \"\"\"\n",
        "    print(\"\\n=== Step 2: Preprocessing Text ===\")\n",
        "\n",
        "    def clean_text(text):\n",
        "        text = re.sub(r'\\s+', ' ', text)\n",
        "        text = re.sub(r'[^\\w\\s.,!?؟،-]', '', text)\n",
        "        return text.strip()\n",
        "\n",
        "    df['cleaned_content'] = df['content'].apply(clean_text)\n",
        "    print(\"Sample of cleaned text (English):\")\n",
        "    print(df[df['language'] == 'en']['cleaned_content'].iloc[0][:200])\n",
        "    print(\"\\nSample of cleaned text (Urdu):\")\n",
        "    print(df[df['language'] == 'ur']['cleaned_content'].iloc[0][:200])\n",
        "\n",
        "    return df\n"
      ],
      "metadata": {
        "id": "Sc67mqgMflbq"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def apply_enhanced_chunking(df):\n",
        "    \"\"\"\n",
        "    Apply multiple chunking strategies with improved handling\n",
        "    \"\"\"\n",
        "    print(\"\\n=== Step 3: Applying Enhanced Chunking Strategies ===\")\n",
        "\n",
        "    def create_fixed_length_chunks(text, chunk_size=500, overlap=50):\n",
        "        chunks = []\n",
        "        start = 0\n",
        "        text_length = len(text)\n",
        "\n",
        "        while start < text_length:\n",
        "            end = start + chunk_size\n",
        "            chunk = text[start:end]\n",
        "            chunks.append(chunk)\n",
        "            start = end - overlap\n",
        "\n",
        "        return chunks\n",
        "\n",
        "    def create_sentence_chunks(text, max_chunk_size=500):\n",
        "        if any('\\u0600' <= char <= '\\u06FF' for char in text):\n",
        "            sentences = re.split(r'[.!?؟،]', text)\n",
        "            sentences = [s.strip() for s in sentences if s.strip()]\n",
        "        else:\n",
        "            try:\n",
        "                sentences = sent_tokenize(text)\n",
        "            except:\n",
        "                sentences = re.split(r'[.!?]', text)\n",
        "                sentences = [s.strip() for s in sentences if s.strip()]\n",
        "\n",
        "        chunks = []\n",
        "        current_chunk = []\n",
        "        current_size = 0\n",
        "\n",
        "        for sentence in sentences:\n",
        "            sentence_size = len(sentence)\n",
        "            if current_size + sentence_size > max_chunk_size and current_chunk:\n",
        "                chunks.append(' '.join(current_chunk))\n",
        "                current_chunk = [sentence]\n",
        "                current_size = sentence_size\n",
        "            else:\n",
        "                current_chunk.append(sentence)\n",
        "                current_size += sentence_size\n",
        "\n",
        "        if current_chunk:\n",
        "            chunks.append(' '.join(current_chunk))\n",
        "\n",
        "        return chunks\n",
        "\n",
        "    def create_semantic_chunks(text, max_chunk_size=500):\n",
        "        \"\"\"\n",
        "        Create chunks based on semantic units (paragraphs, sections)\n",
        "        \"\"\"\n",
        "        paragraphs = text.split('\\n\\n')\n",
        "        chunks = []\n",
        "        current_chunk = []\n",
        "        current_size = 0\n",
        "\n",
        "        for para in paragraphs:\n",
        "            para = para.strip()\n",
        "            if not para:\n",
        "                continue\n",
        "\n",
        "            para_size = len(para)\n",
        "            if current_size + para_size > max_chunk_size and current_chunk:\n",
        "                chunks.append('\\n\\n'.join(current_chunk))\n",
        "                current_chunk = [para]\n",
        "                current_size = para_size\n",
        "            else:\n",
        "                current_chunk.append(para)\n",
        "                current_size += para_size\n",
        "\n",
        "        if current_chunk:\n",
        "            chunks.append('\\n\\n'.join(current_chunk))\n",
        "\n",
        "        return chunks\n",
        "\n",
        "    # Apply all chunking strategies\n",
        "    df['fixed_chunks'] = df['cleaned_content'].apply(lambda x: create_fixed_length_chunks(x))\n",
        "    df['sentence_chunks'] = df['cleaned_content'].apply(lambda x: create_sentence_chunks(x))\n",
        "    df['semantic_chunks'] = df['cleaned_content'].apply(lambda x: create_semantic_chunks(x))\n",
        "\n",
        "    # Print chunking statistics\n",
        "    print(\"\\nChunking Statistics:\")\n",
        "    for strategy in ['fixed_chunks', 'sentence_chunks', 'semantic_chunks']:\n",
        "        total_chunks = sum(len(chunks) for chunks in df[strategy])\n",
        "        avg_chunks = total_chunks / len(df)\n",
        "        print(f\"{strategy}:\")\n",
        "        print(f\"  Total chunks: {total_chunks}\")\n",
        "        print(f\"  Average chunks per document: {avg_chunks:.2f}\")\n",
        "\n",
        "    return df"
      ],
      "metadata": {
        "id": "zbPllIEMjd6p"
      },
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_multilingual_embeddings(df):\n",
        "    \"\"\"\n",
        "    Generate embeddings using LaBSE model\n",
        "    \"\"\"\n",
        "    print(\"\\n=== Step 4: Generating Multilingual Embeddings ===\")\n",
        "\n",
        "    def get_embeddings(texts, model_name, batch_size=32):\n",
        "        print(f\"Loading model: {model_name}\")\n",
        "        model = SentenceTransformer(model_name)\n",
        "\n",
        "        # Verify model dimension\n",
        "        test_embedding = model.encode(\"test\")\n",
        "        embedding_dim = len(test_embedding)\n",
        "        print(f\"Model {model_name} generates {embedding_dim}-dimensional embeddings\")\n",
        "\n",
        "        if embedding_dim != 768:\n",
        "            raise ValueError(f\"Model {model_name} generates {embedding_dim}-dimensional embeddings, but we need 768-dimensional embeddings\")\n",
        "\n",
        "        all_embeddings = []\n",
        "        for i in tqdm(range(0, len(texts), batch_size), desc=f\"Generating embeddings with {model_name}\"):\n",
        "            batch_texts = texts[i:i + batch_size]\n",
        "            batch_embeddings = model.encode(batch_texts, show_progress_bar=False)\n",
        "            all_embeddings.extend(batch_embeddings)\n",
        "\n",
        "        return np.array(all_embeddings)\n",
        "\n",
        "    # Using only LaBSE model for multilingual support\n",
        "    model = 'sentence-transformers/LaBSE'  # Language-agnostic BERT (768d)\n",
        "\n",
        "    chunking_strategies = ['fixed_chunks', 'sentence_chunks', 'semantic_chunks']\n",
        "    embeddings_dict = {}\n",
        "\n",
        "    for strategy in chunking_strategies:\n",
        "        print(f\"\\nProcessing {strategy}...\")\n",
        "        all_chunks = []\n",
        "        for chunks in df[strategy]:\n",
        "            all_chunks.extend(chunks)\n",
        "\n",
        "        print(f\"Total chunks to process: {len(all_chunks)}\")\n",
        "\n",
        "        try:\n",
        "            print(f\"\\nGenerating embeddings with {model}\")\n",
        "            embeddings = get_embeddings(all_chunks, model)\n",
        "\n",
        "            # Verify embedding dimensions\n",
        "            if embeddings.shape[1] != 768:\n",
        "                print(f\"Warning: {model} generated {embeddings.shape[1]}-dimensional embeddings, skipping...\")\n",
        "                continue\n",
        "\n",
        "            key = f\"{strategy}_LaBSE\"\n",
        "            embeddings_dict[key] = embeddings\n",
        "            np.save(f'embeddings_{key}.npy', embeddings)\n",
        "            print(f\"Successfully generated and saved embeddings for {key}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing {model}: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "    if not embeddings_dict:\n",
        "        raise ValueError(\"No embeddings were successfully generated. Please check the model configuration.\")\n",
        "\n",
        "    return embeddings_dict\n"
      ],
      "metadata": {
        "id": "J4LHduNwjg5T"
      },
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def setup_pinecone():\n",
        "    \"\"\"\n",
        "    Connect to Pinecone index\n",
        "    \"\"\"\n",
        "    print(\"\\n=== Step 5: Setting up Pinecone ===\")\n",
        "    try:\n",
        "        # Replace with your Pinecone API key\n",
        "        PINECONE_API_KEY = \"pcsk_ffw9K_3f5Semen1JhXkdPBYdJsZEWmZfCP7T1tsRixFSHb1GGuWiL7ij21K24rsEhTbBo\"\n",
        "        index_name = \"urdu-eng-rag\"\n",
        "        dimension = 768\n",
        "\n",
        "        pc = Pinecone(api_key=PINECONE_API_KEY)\n",
        "\n",
        "        if index_name not in pc.list_indexes().names():\n",
        "            print(f\"Creating new index: {index_name}\")\n",
        "            pc.create_index(\n",
        "                name=index_name,\n",
        "                dimension=dimension,\n",
        "                metric='cosine',\n",
        "                spec=ServerlessSpec(\n",
        "                    cloud='aws',\n",
        "                    region='us-east-1'\n",
        "                )\n",
        "            )\n",
        "            print(\"Index created successfully!\")\n",
        "        else:\n",
        "            print(f\"Using existing index: {index_name}\")\n",
        "\n",
        "        index = pc.Index(index_name)\n",
        "        print(\"Pinecone setup completed successfully!\")\n",
        "        return index\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error setting up Pinecone: {str(e)}\")\n",
        "        raise"
      ],
      "metadata": {
        "id": "RvG6obJejntk"
      },
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def store_enhanced_embeddings(index, df, embeddings_dict):\n",
        "    \"\"\"\n",
        "    Store embeddings with enhanced metadata\n",
        "    \"\"\"\n",
        "    print(\"\\n=== Step 6: Storing Enhanced Embeddings ===\")\n",
        "\n",
        "    def store_embeddings_batch(embeddings, chunks, metadata, batch_size=100):\n",
        "        # Verify embedding dimensions before storing\n",
        "        if embeddings.shape[1] != 768:\n",
        "            raise ValueError(f\"Embedding dimension {embeddings.shape[1]} does not match required dimension 768\")\n",
        "\n",
        "        for i in range(0, len(embeddings), batch_size):\n",
        "            batch_embeddings = embeddings[i:i + batch_size]\n",
        "            batch_chunks = chunks[i:i + batch_size]\n",
        "\n",
        "            vectors = []\n",
        "            for j, (embedding, chunk) in enumerate(zip(batch_embeddings, batch_chunks)):\n",
        "                vector = {\n",
        "                    'id': f'vec_{i+j}',\n",
        "                    'values': embedding.tolist(),\n",
        "                    'metadata': {\n",
        "                        'text': chunk,\n",
        "                        **metadata\n",
        "                    }\n",
        "                }\n",
        "                vectors.append(vector)\n",
        "\n",
        "            try:\n",
        "                index.upsert(vectors=vectors)\n",
        "                print(f\"Successfully stored batch {i//batch_size + 1}\")\n",
        "            except Exception as e:\n",
        "                print(f\"Error storing batch {i//batch_size + 1}: {str(e)}\")\n",
        "                raise\n",
        "\n",
        "    for strategy in ['fixed_chunks', 'sentence_chunks', 'semantic_chunks']:\n",
        "        print(f\"\\nProcessing {strategy}...\")\n",
        "        all_chunks = []\n",
        "        for chunks in df[strategy]:\n",
        "            all_chunks.extend(chunks)\n",
        "\n",
        "        key = f\"{strategy}_LaBSE\"\n",
        "        if key in embeddings_dict:\n",
        "            try:\n",
        "                print(f\"Storing embeddings for {key}\")\n",
        "                embeddings = embeddings_dict[key]\n",
        "\n",
        "                # Verify embedding dimensions\n",
        "                if embeddings.shape[1] != 768:\n",
        "                    print(f\"Warning: {key} has incorrect dimension {embeddings.shape[1]}, skipping...\")\n",
        "                    continue\n",
        "\n",
        "                metadata = {\n",
        "                    'chunking_strategy': strategy,\n",
        "                    'model': 'LaBSE',\n",
        "                    'language': 'multilingual'\n",
        "                }\n",
        "\n",
        "                store_embeddings_batch(embeddings, all_chunks, metadata)\n",
        "                print(f\"Successfully stored all embeddings for {key}\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error storing embeddings for {key}: {str(e)}\")\n",
        "                raise\n",
        "\n",
        "    print(\"All embeddings stored successfully!\")"
      ],
      "metadata": {
        "id": "G9G9Plmwj0jV"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def summarize_context(context_chunks, max_chunk_size=1000):\n",
        "   try:\n",
        "        summaries = []\n",
        "        for chunk in context_chunks:\n",
        "            if len(chunk) > max_chunk_size:\n",
        "                sub_chunks = [chunk[i:i+max_chunk_size] for i in range(0, len(chunk), max_chunk_size)]\n",
        "                chunk_summaries = []\n",
        "                for sub_chunk in sub_chunks:\n",
        "                    prompt = f\"\"\"Summarize the following text, focusing on key information and main points:\n",
        "                    {sub_chunk}\n",
        "                    Summary:\"\"\"\n",
        "\n",
        "                    response = client.chat.completions.create(\n",
        "                        model=\"llama-3.3-70b-versatile\",  # Using Mixtral model from Groq\n",
        "                        messages=[\n",
        "                            {\"role\": \"system\", \"content\": \"You are a helpful summarizer.\"},\n",
        "                            {\"role\": \"user\", \"content\": prompt}\n",
        "                        ],\n",
        "                        max_tokens=150,\n",
        "                        temperature=0.3\n",
        "                    )\n",
        "                    chunk_summaries.append(response.choices[0].message.content.strip())\n",
        "\n",
        "                summaries.append(\" \".join(chunk_summaries))\n",
        "            else:\n",
        "                summaries.append(chunk)\n",
        "\n",
        "        return summaries\n",
        "   except Exception as e:\n",
        "        print(f\"Error in summarize_context: {str(e)}\")\n",
        "        return context_chunks"
      ],
      "metadata": {
        "id": "l1jlJcgmj5JX"
      },
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def rank_context_chunks(chunks, query, model='sentence-transformers/LaBSE'):\n",
        "    \"\"\"\n",
        "    Rank context chunks by relevance to the query\n",
        "    \"\"\"\n",
        "    model = SentenceTransformer(model)\n",
        "    query_embedding = model.encode(query)\n",
        "    chunk_embeddings = model.encode(chunks)\n",
        "\n",
        "    similarities = np.dot(chunk_embeddings, query_embedding) / (\n",
        "        np.linalg.norm(chunk_embeddings, axis=1) * np.linalg.norm(query_embedding)\n",
        "    )\n",
        "\n",
        "    ranked_chunks = [(chunk, score) for chunk, score in zip(chunks, similarities)]\n",
        "    ranked_chunks.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    return ranked_chunks"
      ],
      "metadata": {
        "id": "Vp8b0cgoj88I"
      },
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def filter_redundant_chunks(chunks, similarity_threshold=0.85):\n",
        "    \"\"\"\n",
        "    Remove redundant chunks based on similarity\n",
        "    \"\"\"\n",
        "    model = SentenceTransformer('sentence-transformers/LaBSE')\n",
        "    embeddings = model.encode(chunks)\n",
        "\n",
        "    unique_chunks = []\n",
        "    for i, chunk in enumerate(chunks):\n",
        "        is_redundant = False\n",
        "        for unique_chunk in unique_chunks:\n",
        "            similarity = np.dot(embeddings[i], model.encode(unique_chunk)) / (\n",
        "                np.linalg.norm(embeddings[i]) * np.linalg.norm(model.encode(unique_chunk))\n",
        "            )\n",
        "            if similarity > similarity_threshold:\n",
        "                is_redundant = True\n",
        "                break\n",
        "        if not is_redundant:\n",
        "            unique_chunks.append(chunk)\n",
        "\n",
        "    return unique_chunks"
      ],
      "metadata": {
        "id": "GmTKEkSMj_FV"
      },
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_enhanced_prompt(context, query, system_instruction=None):\n",
        "    \"\"\"\n",
        "    Build an enhanced prompt with better structure and instructions\n",
        "    \"\"\"\n",
        "    if system_instruction is None:\n",
        "        system_instruction = \"\"\"You are a helpful assistant that provides accurate and relevant information.\n",
        "        Use the provided context to answer the question. If the answer is not in the context, say so.\n",
        "        Ensure your response is:\n",
        "        1. Accurate and based on the context\n",
        "        2. Well-structured and easy to understand\n",
        "        3. Comprehensive but concise\n",
        "        4. Relevant to the specific question asked\"\"\"\n",
        "\n",
        "    prompt = f\"\"\"{system_instruction}\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Question: {query}\n",
        "\n",
        "Please provide a detailed answer based on the context above. If the context doesn't contain enough information to answer the question, please state that clearly.\"\"\"\n",
        "\n",
        "    return prompt"
      ],
      "metadata": {
        "id": "bj3_pCxUkBLm"
      },
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_response_quality(response, query, context_chunks):\n",
        "    \"\"\"\n",
        "    Evaluate the quality of the generated response using Groq\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Join context chunks for evaluation\n",
        "        context = \"\\n\\n\".join(context_chunks)\n",
        "        evaluation_prompt = f\"\"\"Evaluate the following response for its quality and relevance:\n",
        "\n",
        "Query: {query}\n",
        "\n",
        "Context: {context}\n",
        "\n",
        "Response: {response}\n",
        "\n",
        "Please evaluate the response on the following criteria:\n",
        "1. Relevance to the query\n",
        "2. Accuracy based on the context\n",
        "3. Completeness of the answer\n",
        "4. Clarity and coherence\n",
        "5. Use of context information\n",
        "\n",
        "Provide a score from 1-10 for each criterion and a brief explanation.\"\"\"\n",
        "\n",
        "        evaluation = client.chat.completions.create(\n",
        "            model=\"llama-3.3-70b-versatile\",\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": \"You are an evaluation expert.\"},\n",
        "                {\"role\": \"user\", \"content\": evaluation_prompt}\n",
        "            ],\n",
        "            max_tokens=300,\n",
        "            temperature=0.3\n",
        "        )\n",
        "\n",
        "        return evaluation.choices[0].message.content.strip()\n",
        "    except Exception as e:\n",
        "        print(f\"Error in evaluate_response_quality: {str(e)}\")\n",
        "        return \"Evaluation failed due to an error.\""
      ],
      "metadata": {
        "id": "mmqK2rMJkGcC"
      },
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def enhanced_rag_pipeline(index, user_query, embed_model='sentence-transformers/LaBSE',\n",
        "                         top_k=20, min_score=0.4, max_tokens=3000):\n",
        "    \"\"\"\n",
        "    Enhanced RAG pipeline with improved context handling and response generation using Groq\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # 1. Embed and retrieve\n",
        "        model = SentenceTransformer(embed_model)\n",
        "        query_embedding = model.encode(user_query)\n",
        "\n",
        "        results = index.query(\n",
        "            vector=query_embedding.tolist(),\n",
        "            top_k=top_k,\n",
        "            include_metadata=True\n",
        "        )\n",
        "\n",
        "        # 2. Filter and process results\n",
        "        matches = [m for m in results.matches if m.score >= min_score]\n",
        "\n",
        "        # 3. Extract and rank context chunks\n",
        "        context_chunks = [m.metadata['text'] for m in matches]\n",
        "        ranked_chunks = rank_context_chunks(context_chunks, user_query)\n",
        "\n",
        "        # 4. Remove redundant chunks\n",
        "        unique_chunks = filter_redundant_chunks([chunk for chunk, _ in ranked_chunks])\n",
        "\n",
        "        # 5. Summarize if needed\n",
        "        if len(unique_chunks) > 10:\n",
        "            summarized_chunks = summarize_context(unique_chunks)\n",
        "        else:\n",
        "            summarized_chunks = unique_chunks\n",
        "\n",
        "        # 6. Build final context\n",
        "        context = \"\\n\\n\".join(summarized_chunks)\n",
        "\n",
        "        # 7. Build enhanced prompt\n",
        "        prompt = build_enhanced_prompt(context, user_query)\n",
        "\n",
        "        # 8. Generate response with improved parameters\n",
        "        response = client.chat.completions.create(\n",
        "            model=\"llama-3.3-70b-versatile\",\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "                {\"role\": \"user\", \"content\": prompt}\n",
        "            ],\n",
        "            max_tokens=512,\n",
        "            temperature=0.3,\n",
        "        )\n",
        "\n",
        "        response_text = response.choices[0].message.content.strip()\n",
        "\n",
        "        # Evaluate response quality\n",
        "        evaluation = evaluate_response_quality(response_text, user_query, summarized_chunks)\n",
        "        print(f\"\\nResponse Evaluation:\\n{evaluation}\")\n",
        "\n",
        "        return response_text\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error in enhanced_rag_pipeline: {str(e)}\")\n",
        "        return \"Sorry, I encountered an error while processing your request.\"\n"
      ],
      "metadata": {
        "id": "8wsjtd-ZkJwL"
      },
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    try:\n",
        "\n",
        "        # Initialize Groq client\n",
        "        groq_api_key = os.getenv(\"GROQ_API_KEY\")\n",
        "        if not groq_api_key:\n",
        "            raise ValueError(\"GROQ_API_KEY environment variable is not set\")\n",
        "\n",
        "        global client\n",
        "        client = Groq(api_key=groq_api_key)\n",
        "        # Step 1: Collect multilingual data\n",
        "        df = collect_multilingual_data()\n",
        "\n",
        "        # Step 2: Preprocess text\n",
        "        df = preprocess_text(df)\n",
        "\n",
        "        # Step 3: Apply enhanced chunking\n",
        "        df = apply_enhanced_chunking(df)\n",
        "\n",
        "        # Step 4: Generate multilingual embeddings\n",
        "        embeddings_dict = generate_multilingual_embeddings(df)\n",
        "\n",
        "        # Step 5: Setup Pinecone\n",
        "        index = setup_pinecone()\n",
        "\n",
        "        # Step 6: Store enhanced embeddings\n",
        "        store_enhanced_embeddings(index, df, embeddings_dict)\n",
        "\n",
        "        # Step 7: Test enhanced RAG pipeline\n",
        "        test_queries = [\n",
        "            \"What are the health benefits of Pakistani cuisine?\",\n",
        "            \"پاکستانی کھانوں کے صحت کے فوائد کیا ہیں؟\",\n",
        "            \"How can traditional Pakistani food be part of a balanced diet?\",\n",
        "            \"روایتی پاکستانی کھانوں کو متوازن غذا کا حصہ کیسے بنایا جا سکتا ہے؟\"\n",
        "        ]\n",
        "\n",
        "        for query in test_queries:\n",
        "            print(f\"\\nQuery: {query}\")\n",
        "            response = enhanced_rag_pipeline(index, query)\n",
        "            print(f\"Response: {response}\")\n",
        "\n",
        "\n",
        "        print(\"\\nAll steps completed successfully!\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {str(e)}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1pwUbgHpkQ7R",
        "outputId": "99dfe816-09c5-49ee-a0de-19f68af76c04"
      },
      "execution_count": 94,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== Step 1: Collecting Multilingual Data ===\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Collecting English Wikipedia articles: 100%|██████████| 10/10 [00:01<00:00,  6.28it/s]\n",
            "Collecting Urdu Wikipedia articles:  67%|██████▋   | 2/3 [00:00<00:00,  7.14it/s]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Warning: No content found for Pakistani_cuisine in ur\n",
            "Warning: No content found for Indian_cuisine in ur\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Collecting Urdu Wikipedia articles: 100%|██████████| 3/3 [00:00<00:00,  7.03it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Warning: No content found for Traditional_medicine in ur\n",
            "\n",
            "Data Collection Summary:\n",
            "Total documents: 13\n",
            "English documents: 11\n",
            "Urdu documents: 2\n",
            "Wikipedia sources: 9\n",
            "Custom sources: 4\n",
            "\n",
            "=== Step 2: Preprocessing Text ===\n",
            "Sample of cleaned text (English):\n",
            "Nutrition is the biochemical and physiological process by which an organism uses food and water to support its life. The intake of these substances provides organisms with nutrients divided into macro\n",
            "\n",
            "Sample of cleaned text (Urdu):\n",
            "پاکستانی کھانوں میں استعمال ہونے والے مصالحے صحت کے لیے بہت فائدہ مند ہیں ہلدی میں اینٹی انفلیمیٹری خصوصیات ہیں، زیرہ ہاضمہ کو بہتر بناتا ہے، دھنیا اینٹی آکسیڈنٹس سے بھرپور ہے، الائچی سانس کی بیماریوں\n",
            "\n",
            "=== Step 3: Applying Enhanced Chunking Strategies ===\n",
            "\n",
            "Chunking Statistics:\n",
            "fixed_chunks:\n",
            "  Total chunks: 41\n",
            "  Average chunks per document: 3.15\n",
            "sentence_chunks:\n",
            "  Total chunks: 40\n",
            "  Average chunks per document: 3.08\n",
            "semantic_chunks:\n",
            "  Total chunks: 13\n",
            "  Average chunks per document: 1.00\n",
            "\n",
            "=== Step 4: Generating Multilingual Embeddings ===\n",
            "\n",
            "Processing fixed_chunks...\n",
            "Total chunks to process: 41\n",
            "\n",
            "Generating embeddings with sentence-transformers/LaBSE\n",
            "Loading model: sentence-transformers/LaBSE\n",
            "Model sentence-transformers/LaBSE generates 768-dimensional embeddings\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Generating embeddings with sentence-transformers/LaBSE: 100%|██████████| 2/2 [00:14<00:00,  7.32s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Successfully generated and saved embeddings for fixed_chunks_LaBSE\n",
            "\n",
            "Processing sentence_chunks...\n",
            "Total chunks to process: 40\n",
            "\n",
            "Generating embeddings with sentence-transformers/LaBSE\n",
            "Loading model: sentence-transformers/LaBSE\n",
            "Model sentence-transformers/LaBSE generates 768-dimensional embeddings\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Generating embeddings with sentence-transformers/LaBSE: 100%|██████████| 2/2 [00:13<00:00,  6.94s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Successfully generated and saved embeddings for sentence_chunks_LaBSE\n",
            "\n",
            "Processing semantic_chunks...\n",
            "Total chunks to process: 13\n",
            "\n",
            "Generating embeddings with sentence-transformers/LaBSE\n",
            "Loading model: sentence-transformers/LaBSE\n",
            "Model sentence-transformers/LaBSE generates 768-dimensional embeddings\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Generating embeddings with sentence-transformers/LaBSE: 100%|██████████| 1/1 [00:13<00:00, 13.17s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully generated and saved embeddings for semantic_chunks_LaBSE\n",
            "\n",
            "=== Step 5: Setting up Pinecone ===\n",
            "Using existing index: urdu-eng-rag\n",
            "Pinecone setup completed successfully!\n",
            "\n",
            "=== Step 6: Storing Enhanced Embeddings ===\n",
            "\n",
            "Processing fixed_chunks...\n",
            "Storing embeddings for fixed_chunks_LaBSE\n",
            "Successfully stored batch 1\n",
            "Successfully stored all embeddings for fixed_chunks_LaBSE\n",
            "\n",
            "Processing sentence_chunks...\n",
            "Storing embeddings for sentence_chunks_LaBSE\n",
            "Successfully stored batch 1\n",
            "Successfully stored all embeddings for sentence_chunks_LaBSE\n",
            "\n",
            "Processing semantic_chunks...\n",
            "Storing embeddings for semantic_chunks_LaBSE\n",
            "Successfully stored batch 1\n",
            "Successfully stored all embeddings for semantic_chunks_LaBSE\n",
            "All embeddings stored successfully!\n",
            "\n",
            "Query: What are the health benefits of Pakistani cuisine?\n",
            "\n",
            "Response Evaluation:\n",
            "Here's the evaluation of the response based on the given criteria:\n",
            "\n",
            "1. **Relevance to the query: 9**\n",
            "The response is highly relevant to the query as it directly addresses the health benefits of Pakistani cuisine, specifically highlighting the benefits of various spices used in Pakistani cooking. The response stays focused on the topic and provides relevant information.\n",
            "\n",
            "2. **Accuracy based on the context: 9**\n",
            "The response is accurate based on the context provided. The health benefits of the spices mentioned, such as turmeric, cumin, coriander, cardamom, and cinnamon, are consistent with the information provided in the context. The response does not introduce any contradictory or incorrect information.\n",
            "\n",
            "3. **Completeness of the answer: 8**\n",
            "The response provides a comprehensive list of spices and their health benefits, which is a significant aspect of Pakistani cuisine. However, it could be more complete by discussing other health benefits of Pakistani cuisine, such as the use of herbs, vegetables, and whole grains. The response touches on cooking methods but could delve deeper into their nutritional benefits.\n",
            "\n",
            "4. **Clarity and coherence: 9**\n",
            "The response is clear and well-structured, making it easy to follow and understand. The use of headings and numbered lists enhances the clarity and coherence of the response. The language is straightforward, and the explanations are concise, allowing the reader to quickly grasp the information.\n",
            "\n",
            "5. **Use of context information: 8**\n",
            "The response effectively uses the context information to provide\n",
            "Response: پاکستانی کھانوں میں استعمال ہونے والے مختلف مصالحے صحت کے لیے بہت فائدہ مند ہیں۔ ان میں سے کچھ اہم مصالحے اور ان کے فوائد یہ ہیں:\n",
            "\n",
            "1. **ہلدی (Turmeric)**: ہلدی میں اینٹی انفلیمیٹری خصوصیات ہیں جو جسم میں سوزش کو کم کرنے میں مدد کرتی ہیں۔\n",
            "2. **زیرہ (Cumin)**: زیرہ ہاضمہ کو بہتر بناتا ہے اور پیٹ کے مسائل کو حل کرنے میں مدد کرتا ہے۔\n",
            "3. **دھنیا (Coriander)**: دھنیا اینٹی آکسیڈنٹس سے بھرپور ہے جو جسم کو آکسیڈیٹو دباؤ سے بچاتا ہے اور عمر بڑھنے کے اثرات کو کم کرتا ہے۔\n",
            "4. **الائچی (Cardamom)**: الائچی سانس کی بیماریوں میں مدد کرتی ہے اور جسم کے مختلف نظاموں کو بہتر بناتی ہے۔\n",
            "5. **دارچینی (Cinnamon)**: دارچینی خون میں شکر کی سطح کو کنٹرول کرتی ہے جو ذیابیطس کے مریضوں کے لیے بہت فائدہ مند ہے۔\n",
            "\n",
            "پاکستانی کھانوں کے طریقے بھی غذائیت کو برقرار رکھنے میں اہم کردار ادا کرتے ہیں۔ ان میں **دم پخت\n",
            "\n",
            "Query: پاکستانی کھانوں کے صحت کے فوائد کیا ہیں؟\n",
            "\n",
            "Response Evaluation:\n",
            "Here's the evaluation of the response based on the given criteria:\n",
            "\n",
            "1. **Relevance to the query: 9/10**\n",
            "The response is highly relevant to the query, as it directly addresses the health benefits of Pakistani cuisine and the spices used in it. However, it could be improved by providing more specific examples of Pakistani dishes that incorporate these spices.\n",
            "\n",
            "2. **Accuracy based on the context: 9/10**\n",
            "The response is accurate based on the context provided, as it correctly identifies the health benefits of various spices commonly used in Pakistani cuisine. However, it could be improved by providing more detailed information about the specific health benefits of each spice.\n",
            "\n",
            "3. **Completeness of the answer: 8/10**\n",
            "The response provides a good overview of the health benefits of Pakistani spices and cooking methods, but it could be more comprehensive. For example, it could discuss the importance of using fresh ingredients, the role of fermentation in Pakistani cuisine, or the health benefits of specific Pakistani dishes.\n",
            "\n",
            "4. **Clarity and coherence: 9/10**\n",
            "The response is clear and well-organized, making it easy to follow and understand. The use of headings and bullet points helps to break up the text and highlight key points. However, some of the sentences could be rephrased for better clarity and flow.\n",
            "\n",
            "5. **Use of context information: 8/10**\n",
            "The response uses the context information provided to support its claims about the health benefits of Pakistani spices and cooking methods\n",
            "Response: پاکستانی کھانوں میں استعمال ہونے والے مصالحے صحت کے لیے بہت فائدہ مند ہیں۔ ان میں سے کچھ اہم فوائد یہ ہیں:\n",
            "\n",
            "1. **ہلدی**: ہلدی میں اینٹی انفلیمیٹری خصوصیات ہیں، جو جسم میں سوزش کو کم کرنے میں مدد کرتی ہیں۔\n",
            "2. **زیرہ**: زیرہ ہاضمہ کو بہتر بناتا ہے، جو کہ پیٹ کے مسائل کو حل کرنے میں مدد کرتا ہے۔\n",
            "3. **دھنیا**: دھنیا اینٹی آکسیڈنٹس سے بھرپور ہے، جو کہ جسم کو آکسیڈیٹو دباؤ سے بچاتا ہے۔\n",
            "4. **الائچی**: الائچی سانس کی بیماریوں میں مدد کرتی ہے، جو کہ سانس لینے میں آسانی پیدا کرتی ہے۔\n",
            "5. **دارچینی**: دارچینی خون میں شکر کی سطح کو کنٹرول کرتی ہے، جو کہ ذیابیطس کے مریضوں کے لیے فائدہ مند ہے۔\n",
            "\n",
            "اس کے علاوہ، روایتی پاکستانی کھانوں کے طریقے بھی غذائیت کو برقرار رکھتے ہیں اور ذائقہ بڑھاتے ہیں۔ ان میں سے کچھ اہم طریقے یہ ہیں:\n",
            "\n",
            "1. **دم پخت**: مہر بند برتنو\n",
            "\n",
            "Query: How can traditional Pakistani food be part of a balanced diet?\n",
            "\n",
            "Response Evaluation:\n",
            "Here's the evaluation of the response based on the given criteria:\n",
            "\n",
            "1. **Relevance to the query: 8/10**\n",
            "The response is relevant to the query as it discusses how traditional Pakistani food can be part of a balanced diet. However, it could be more focused on providing specific examples and explanations of how Pakistani cuisine contributes to a balanced diet.\n",
            "\n",
            "2. **Accuracy based on the context: 9/10**\n",
            "The response is accurate based on the context provided, as it mentions the health benefits of spices used in Pakistani cuisine, the variety of fruits, vegetables, and grains used, and traditional cooking methods. However, it could be more accurate if it provided more specific examples and data to support its claims.\n",
            "\n",
            "3. **Completeness of the answer: 7/10**\n",
            "The response provides some good points about the health benefits of Pakistani cuisine, but it could be more comprehensive. For example, it could discuss the importance of portion control, the role of dairy products and meats in Pakistani cuisine, and how to balance different food groups in a traditional Pakistani diet.\n",
            "\n",
            "4. **Clarity and coherence: 8/10**\n",
            "The response is generally clear and coherent, with each paragraph focusing on a specific aspect of Pakistani cuisine. However, the transitions between paragraphs could be smoother, and the response could benefit from a clearer introduction and conclusion.\n",
            "\n",
            "5. **Use of context information: 8/10**\n",
            "The response uses some context information, such as the health benefits of spices and the\n",
            "Response: پاکستانی کھانوں کو توازن شدہ غذا کا حصہ بنانے کے لیے کئی وجوہات ہیں۔ پہلی وجہ یہ ہے کہ پاکستانی کھانوں میں استعمال ہونے والے مصالحے صحت کے لیے بہت فائدہ مند ہیں۔ ہلدی میں اینٹی انفلیمیٹری خصوصیات ہیں، زیرہ ہاضمہ کو بہتر بناتا ہے، دھنیا اینٹی آکسیڈنٹس سے بھرپور ہے، الائچی سانس کی بیماریوں میں مدد کرتی ہے، اور دارچینی خون میں شکر کی سطح کو کنٹرول کرتی ہے۔\n",
            "\n",
            "دوسری وجہ یہ ہے کہ پاکستانی کھانوں میں مختلف قسم کے پھل، سبزیاں، اور اناج استعمال ہوتے ہیں جو ضروری غذائی اجزاء سے بھرپور ہوتے ہیں۔ مثال کے طور پر، پاکستانی کھانوں میں چاول، گندم، اور دالیں استعمال ہوتی ہیں جو کاربوہائیڈریٹس، پروٹین، اور فائبر سے بھرپور ہوتے ہیں۔\n",
            "\n",
            "تیسری وجہ یہ ہے کہ پاکستانی کھانوں میں روایتی طریقے استعمال ہوتے ہیں جو غذائیت کو برقرار رکھتے ہیں اور ذائقہ بڑھاتے ہیں۔ مثال ک\n",
            "\n",
            "Query: روایتی پاکستانی کھانوں کو متوازن غذا کا حصہ کیسے بنایا جا سکتا ہے؟\n",
            "\n",
            "Response Evaluation:\n",
            "Here's the evaluation of the response based on the given criteria:\n",
            "\n",
            "1. **Relevance to the query: 8/10**\n",
            "The response is relevant to the query as it discusses how to make traditional Pakistani cuisine a part of a balanced diet. However, it could be more focused on providing specific examples or tips on how to achieve this balance.\n",
            "\n",
            "2. **Accuracy based on the context: 9/10**\n",
            "The response is accurate based on the context provided, as it mentions the benefits of using spices and traditional cooking methods in Pakistani cuisine. However, it could be more comprehensive in its discussion of the nutritional aspects of Pakistani food.\n",
            "\n",
            "3. **Completeness of the answer: 7/10**\n",
            "The response provides some useful points about the benefits of spices and traditional cooking methods, but it does not fully address the query. It could be more complete by providing more specific examples of balanced meals, nutritional information, or tips on how to incorporate Pakistani cuisine into a healthy diet.\n",
            "\n",
            "4. **Clarity and coherence: 8/10**\n",
            "The response is clear and coherent, with each point building on the previous one to provide a logical discussion of how to make traditional Pakistani cuisine a part of a balanced diet. However, the transition between points could be smoother, and the response could benefit from more cohesive language.\n",
            "\n",
            "5. **Use of context information: 8/10**\n",
            "The response uses some context information, such as the benefits of spices and traditional cooking methods, to support its points.\n",
            "Response: روایتی پاکستانی کھانوں کو متوازن غذا کا حصہ بنانے کے لیے کچھ اہم نکات پر غور کرنا ضروری ہے۔\n",
            "\n",
            "1. **مصالحے کا استعمال**: پاکستانی کھانوں میں استعمال ہونے والے مصالحے صحت کے لیے بہت فائدہ مند ہیں۔ ہلدی میں اینٹی انفلیمیٹری خصوصیات ہیں، زیرہ ہاضمہ کو بہتر بناتا ہے، دھنیا اینٹی آکسیڈنٹس سے بھرپور ہے، الائچی سانس کی بیماریوں میں مدد کرتی ہے، اور دارچینی خون میں شکر کی سطح کو کنٹرول کرتی ہے۔ لہذا، ان مصالحوں کا استعمال کھانوں میں کرنا چاہیے۔\n",
            "\n",
            "2. **پکانے کے طریقے**: روایتی پاکستانی کھانوں کے پکانے کے طریقے، جیسے کہ دم پخت، تندوری، بھوننا، اور دھونیار، غذائیت کو برقرار رکھتے ہیں اور ذائقہ بڑھاتے ہیں۔ ان طریقوں کا استعمال کھانوں کو پکانے کے لیے کرنا چاہیے۔\n",
            "\n",
            "3. **غذائی اجزاء کا توازن**: پاکستانی کھانوں میں گوشت، سبزیاں، اور دالیں شامل ہوتی ہیں۔ لہذا، ان اجزاء کا توازن رکھنا ضروری ہے تاکہ کھانا متوازن\n",
            "\n",
            "All steps completed successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TJD7pL8Xnhhx"
      },
      "execution_count": 71,
      "outputs": []
    }
  ]
}